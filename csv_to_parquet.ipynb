{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ZxCb91BtRecgV4ymmsM-n6eC3anTC9h6",
      "authorship_tag": "ABX9TyOyEVxUbqG/a+DoaDYJny55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoLeonardoPaez/big_data/blob/main/csv_to_parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Installing Spark and Dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Covid19Data\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "CFPnTCkrhepS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "21efc7f6-d01b-4502-9c46-1dfc194225ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7afbe0e3fdc0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c54b5bafb3fa:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Covid19Data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y5P2AXPZbG9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1a5973-e039-4865-f004-a128920c5d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found /content/drive/MyDrive/covid directory.\n",
            "Found 6 CSV file(s) in the directory.\n",
            "Column 'period_id' created in the DataFrame 'day_wise_df'.\n",
            "All DataFrames successfully written to Parquet format.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from os import walk\n",
        "from pathlib import Path\n",
        "from pyspark.sql.functions import to_date, date_format\n",
        "\n",
        "# Function to validate if the input_dir exist\n",
        "def directory_exist(input_dir):\n",
        "  p = Path(input_dir)\n",
        "  return p.exists()\n",
        "\n",
        "# Function to list CSV files in a directory\n",
        "def list_csv_files(input_dir):\n",
        "    csv_files = []\n",
        "    for (input_dir, dir_names, file_names) in os.walk(input_dir):\n",
        "        for file in file_names:\n",
        "            if file.lower().endswith('.csv'):\n",
        "                csv_files.append(os.path.join(input_dir, file))\n",
        "    if len(csv_files) == 0:\n",
        "        raise Exception(\"No CSV files found in the directory\")\n",
        "    return csv_files\n",
        "\n",
        "# Function to read CSV files and infer schema\n",
        "def read_csv_files(spark_session, files):\n",
        "    return {f'{os.path.splitext(os.path.basename(file))[0]}_df': spark_session.read.options(delimiter=\",\", header=True).option(\"inferSchema\", \"true\").csv(file) for file in files}\n",
        "\n",
        "# Function to create a 'period_id' column based on the 'Date' column\n",
        "# period_id field will be useful to create the dataframe partition\n",
        "def create_period_id_column(df):\n",
        "    df = df.withColumn('Date', to_date('Date', 'yyyy-MM-dd'))\n",
        "    df = df.withColumn('period_id', date_format('Date', 'yyyy-MM'))\n",
        "    return df\n",
        "\n",
        "# Function to clean column names by replacing spaces, special characters, etc\n",
        "def clean_column_names(df):\n",
        "    for column in df.columns:\n",
        "        new_column = column.replace(' ', '_')\n",
        "        new_column = new_column.replace('.', '_')\n",
        "        new_column = new_column.replace('-', '_')\n",
        "        new_column = new_column.replace('/', '_')\n",
        "        new_column = new_column.replace('(', '')\n",
        "        new_column = new_column.replace(')', '')\n",
        "        new_column = new_column.replace(',', '_')\n",
        "        df = df.withColumnRenamed(column, new_column)\n",
        "    return df\n",
        "\n",
        "# Function to write DataFrame to Parquet format with partitioning\n",
        "def write_to_parquet(df, output_dir, partition_column, key_df):\n",
        "    output_path = os.path.join(output_dir, key_df + '.parquet')\n",
        "    df.write.mode('overwrite').partitionBy(partition_column).parquet(output_path)\n",
        "\n",
        "# Main function to execute the data processing and Parquet writing\n",
        "def main():\n",
        "    input_dir = \"/content/drive/MyDrive/covid\"\n",
        "\n",
        "    dir = directory_exist(input_dir)\n",
        "\n",
        "    if dir == False:\n",
        "      raise Exception(\"input_dir does not define. Program terminated\")\n",
        "      return\n",
        "\n",
        "    print(f\"Found directory {input_dir}\")\n",
        "\n",
        "    files = list_csv_files(input_dir)\n",
        "\n",
        "    if not files:\n",
        "        raise Exception(\"No CSV files found in the directory. Program terminated\")\n",
        "        return\n",
        "\n",
        "    dfs = read_csv_files(spark, files)\n",
        "    print(f\"Found {len(dfs)} CSV file(s) in the directory.\")\n",
        "\n",
        "    # Code to create 'period_id' in the DataFrame 'day_wise_df'\n",
        "    day_wise_df = dfs.get('day_wise_df', None)\n",
        "    if day_wise_df is not None:\n",
        "        day_wise_df = create_period_id_column(day_wise_df)\n",
        "        dfs['day_wise_df'] = day_wise_df\n",
        "        print(\"Column 'period_id' created in the DataFrame 'day_wise_df'.\")\n",
        "\n",
        "    output_dir = \"/content/drive/MyDrive/covid/parquet_output\"\n",
        "\n",
        "    # Dictionary that maps each key to the desired partitioning column name\n",
        "    partition_column_dict = {\n",
        "        'country_wise_latest_df': 'Country_Region',\n",
        "        'full_grouped_df': 'Country_Region',\n",
        "        'worldometer_data_df': 'Country_Region',\n",
        "        'covid_19_clean_complete_df': 'Country_Region',\n",
        "        'usa_county_wise_df': 'Province_State',\n",
        "        'day_wise_df': 'period_id'\n",
        "    }\n",
        "\n",
        "    for key_df, df in dfs.items():\n",
        "        df = clean_column_names(df)\n",
        "        partition_column = partition_column_dict.get(key_df, None)\n",
        "        if partition_column is not None:\n",
        "            write_to_parquet(df, output_dir, partition_column, key_df)\n",
        "    print(\"All DataFrames successfully written to Parquet format.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}